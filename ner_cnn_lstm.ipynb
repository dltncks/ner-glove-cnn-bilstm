{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Pre-trained GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings from the given directory\n",
    "def load_glove_embeddings(file_path, embedding_dim=300):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]  # First word is the token\n",
    "            coeffs = np.asarray(values[1:], dtype=\"float32\")  # Remaining values are the embedding\n",
    "            embeddings_index[word] = coeffs  # Store word vector\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe 300D embeddings\n",
    "glove_path = \"C:/Users/CommAdmin/Downloads/glove.6B/glove.6B.300d.txt\"\n", # Change this to your own directory
    "glove_embeddings = load_glove_embeddings(glove_path, embedding_dim=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the CoNLL-2003 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af04080e3c3742e9acd484dca8045400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb1532282c044e088da0dc8863bb493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719a1de409814b728c5d190efcebb61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06b47f6a5f7431ebe846b95f2f3b77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "\n",
    "# Print dataset info to confirm it loaded correctly\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Words and Labels to Numerical Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary from GloVe embeddings\n",
    "word_to_idx = {word: idx + 1 for idx, word in enumerate(glove_embeddings.keys())}\n",
    "word_to_idx[\"<PAD>\"] = 0  # Padding token\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Label mapping\n",
    "label_to_idx = {\"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-ORG\": 3, \"I-ORG\": 4, \"B-LOC\": 5, \"I-LOC\": 6, \"B-MISC\": 7, \"I-MISC\": 8}\n",
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, dataset, word_to_idx, max_len=50):\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "\n",
    "        for item in dataset:\n",
    "            words = item[\"tokens\"]  # Extract words\n",
    "            labels = item[\"ner_tags\"]  # Extract labels (already numeric)\n",
    "\n",
    "            # Convert words to indices using GloVe vocabulary\n",
    "            word_indices = [word_to_idx.get(word.lower(), word_to_idx[\"<PAD>\"]) for word in words]\n",
    "\n",
    "            # Store sentences and labels without forcing fixed length\n",
    "            self.sentences.append(torch.tensor(word_indices, dtype=torch.long))\n",
    "            self.labels.append(torch.tensor(labels, dtype=torch.long))  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.labels[idx]\n",
    "\n",
    "# Function to dynamically pad sequences in a batch\n",
    "def collate_fn(batch):\n",
    "    sentences, labels = zip(*batch)\n",
    "\n",
    "    # Pad sentences and labels to the length of the longest sequence in the batch\n",
    "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=word_to_idx[\"<PAD>\"])\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=0)  # \"O\" label padding\n",
    "\n",
    "    return sentences_padded, labels_padded\n",
    "\n",
    "# Create dataset and DataLoader with collate_fn\n",
    "train_data = NERDataset(dataset[\"train\"], word_to_idx)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define CNN + BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Convert GloVe embeddings into a single NumPy array\n",
    "embedding_dim = 300\n",
    "glove_matrix = np.array(list(glove_embeddings.values()), dtype=np.float32)\n",
    "\n",
    "# Add a padding vector (all zeros) at index 0\n",
    "pad_vector = np.zeros((1, embedding_dim), dtype=np.float32)\n",
    "glove_matrix = np.vstack([pad_vector, glove_matrix])  # Now has shape (400001, 300)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "glove_tensor = torch.tensor(glove_matrix)\n",
    "\n",
    "class CNN_BiLSTM_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, glove_tensor, kernel_size=3):\n",
    "        super(CNN_BiLSTM_NER, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Load pre-trained GloVe embeddings\n",
    "        self.embedding.weight.data.copy_(glove_tensor)\n",
    "        self.embedding.weight.requires_grad = False  # Freeze embeddings\n",
    "\n",
    "        # CNN Layer\n",
    "        self.conv1d = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        # BiLSTM Layer\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)  # Reshape for CNN\n",
    "        x = torch.relu(self.conv1d(x))  # Apply CNN\n",
    "        x = x.permute(0, 2, 1)  # Reshape for LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # BiLSTM processing\n",
    "        output = self.fc(self.dropout(lstm_out))\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(word_to_idx)\n",
    "hidden_dim = 256\n",
    "output_dim = len(label_to_idx)\n",
    "\n",
    "model = CNN_BiLSTM_NER(vocab_size, embedding_dim, hidden_dim, output_dim, glove_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.14063098855265724\n",
      "Epoch 2, Loss: 0.039961655431157636\n",
      "Epoch 3, Loss: 0.028411405298918283\n",
      "Epoch 4, Loss: 0.019972107743373425\n",
      "Epoch 5, Loss: 0.015273114246292917\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for words, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(words)\n",
    "\n",
    "            # Compute loss\n",
    "            outputs = outputs.view(-1, output_dim)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.85      0.89      0.87      1668\n",
      "        MISC       0.67      0.70      0.68       702\n",
      "         ORG       0.73      0.78      0.76      1661\n",
      "         PER       0.91      0.86      0.88      1617\n",
      "\n",
      "   micro avg       0.81      0.83      0.82      5648\n",
      "   macro avg       0.79      0.81      0.80      5648\n",
      "weighted avg       0.81      0.83      0.82      5648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for words, labels in DataLoader(dataset, batch_size=1, collate_fn=collate_fn):\n",
    "            outputs = model(words).argmax(dim=-1).numpy().tolist()\n",
    "            labels = labels.numpy().tolist()\n",
    "\n",
    "            predictions.extend(outputs)\n",
    "            true_labels.extend(labels)\n",
    "\n",
    "    pred_labels = [[idx_to_label[idx] for idx in seq] for seq in predictions]\n",
    "    true_labels = [[idx_to_label[idx] for idx in seq] for seq in true_labels]\n",
    "\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_data = NERDataset(dataset[\"test\"], word_to_idx)\n",
    "evaluate_model(model, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
